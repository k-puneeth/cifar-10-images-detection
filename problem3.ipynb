{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "import pickle\n",
    "from scipy.cluster.vq import vq\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from __future__ import print_function\n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread\n",
    "import platform\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(f):\n",
    "    version = platform.python_version_tuple()\n",
    "    if version[0] == '2':\n",
    "        return  pickle.load(f)\n",
    "    elif version[0] == '3':\n",
    "        return  pickle.load(f, encoding='latin1')\n",
    "    raise ValueError(\"invalid python version: {}\".format(version))\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "  \"\"\" load single batch of cifar \"\"\"\n",
    "  with open(filename, 'rb') as f:\n",
    "    datadict = load_pickle(f)\n",
    "    X = datadict['data']\n",
    "    Y = datadict['labels']\n",
    "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "    Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "  \"\"\" load all of cifar \"\"\"\n",
    "  xs = []\n",
    "  ys = []\n",
    "  for b in range(1,6):\n",
    "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "    X, Y = load_CIFAR_batch(f)\n",
    "    xs.append(X)\n",
    "    ys.append(Y)    \n",
    "  Xtr = np.concatenate(xs)\n",
    "  Ytr = np.concatenate(ys)\n",
    "  del X, Y\n",
    "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "  return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000,\n",
    "                     subtract_mean=True):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for classifiers. These are the same steps as we used for the SVM, but\n",
    "    condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    if subtract_mean:\n",
    "      mean_image = np.mean(X_train, axis=0)\n",
    "      X_train -= mean_image\n",
    "      X_val -= mean_image\n",
    "      X_test -= mean_image\n",
    "    \n",
    "    # Transpose so that channels come first\n",
    "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
    "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
    "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
    "\n",
    "    # Package data into a dictionary\n",
    "    return {\n",
    "      'X_train': X_train, 'y_train': y_train,\n",
    "      'X_val': X_val, 'y_val': y_val,\n",
    "      'X_test': X_test, 'y_test': y_test,\n",
    "    }\n",
    "    \n",
    "\n",
    "def load_tiny_imagenet(path, dtype=np.float32, subtract_mean=True):\n",
    "  \"\"\"\n",
    "  Load TinyImageNet. Each of TinyImageNet-100-A, TinyImageNet-100-B, and\n",
    "  TinyImageNet-200 have the same directory structure, so this can be used\n",
    "  to load any of them.\n",
    "\n",
    "  Inputs:\n",
    "  - path: String giving path to the directory to load.\n",
    "  - dtype: numpy datatype used to load the data.\n",
    "  - subtract_mean: Whether to subtract the mean training image.\n",
    "\n",
    "  Returns: A dictionary with the following entries:\n",
    "  - class_names: A list where class_names[i] is a list of strings giving the\n",
    "    WordNet names for class i in the loaded dataset.\n",
    "  - X_train: (N_tr, 3, 64, 64) array of training images\n",
    "  - y_train: (N_tr,) array of training labels\n",
    "  - X_val: (N_val, 3, 64, 64) array of validation images\n",
    "  - y_val: (N_val,) array of validation labels\n",
    "  - X_test: (N_test, 3, 64, 64) array of testing images.\n",
    "  - y_test: (N_test,) array of test labels; if test labels are not available\n",
    "    (such as in student code) then y_test will be None.\n",
    "  - mean_image: (3, 64, 64) array giving mean training image\n",
    "  \"\"\"\n",
    "  # First load wnids\n",
    "  with open(os.path.join(path, 'wnids.txt'), 'r') as f:\n",
    "    wnids = [x.strip() for x in f]\n",
    "\n",
    "  # Map wnids to integer labels\n",
    "  wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}\n",
    "\n",
    "  # Use words.txt to get names for each class\n",
    "  with open(os.path.join(path, 'words.txt'), 'r') as f:\n",
    "    wnid_to_words = dict(line.split('\\t') for line in f)\n",
    "    for wnid, words in wnid_to_words.iteritems():\n",
    "      wnid_to_words[wnid] = [w.strip() for w in words.split(',')]\n",
    "  class_names = [wnid_to_words[wnid] for wnid in wnids]\n",
    "\n",
    "  # Next load training data.\n",
    "  X_train = []\n",
    "  y_train = []\n",
    "  for i, wnid in enumerate(wnids):\n",
    "    if (i + 1) % 20 == 0:\n",
    "      print('loading training data for synset %d / %d' % (i + 1, len(wnids)))\n",
    "    # To figure out the filenames we need to open the boxes file\n",
    "    boxes_file = os.path.join(path, 'train', wnid, '%s_boxes.txt' % wnid)\n",
    "    with open(boxes_file, 'r') as f:\n",
    "      filenames = [x.split('\\t')[0] for x in f]\n",
    "    num_images = len(filenames)\n",
    "    \n",
    "    X_train_block = np.zeros((num_images, 3, 64, 64), dtype=dtype)\n",
    "    y_train_block = wnid_to_label[wnid] * np.ones(num_images, dtype=np.int64)\n",
    "    for j, img_file in enumerate(filenames):\n",
    "      img_file = os.path.join(path, 'train', wnid, 'images', img_file)\n",
    "      img = imread(img_file)\n",
    "      if img.ndim == 2:\n",
    "        ## grayscale file\n",
    "        img.shape = (64, 64, 1)\n",
    "      X_train_block[j] = img.transpose(2, 0, 1)\n",
    "    X_train.append(X_train_block)\n",
    "    y_train.append(y_train_block)\n",
    "      \n",
    "  # We need to concatenate all training data\n",
    "  X_train = np.concatenate(X_train, axis=0)\n",
    "  y_train = np.concatenate(y_train, axis=0)\n",
    "  \n",
    "  # Next load validation data\n",
    "  with open(os.path.join(path, 'val', 'val_annotations.txt'), 'r') as f:\n",
    "    img_files = []\n",
    "    val_wnids = []\n",
    "    for line in f:\n",
    "      img_file, wnid = line.split('\\t')[:2]\n",
    "      img_files.append(img_file)\n",
    "      val_wnids.append(wnid)\n",
    "    num_val = len(img_files)\n",
    "    y_val = np.array([wnid_to_label[wnid] for wnid in val_wnids])\n",
    "    X_val = np.zeros((num_val, 3, 64, 64), dtype=dtype)\n",
    "    for i, img_file in enumerate(img_files):\n",
    "      img_file = os.path.join(path, 'val', 'images', img_file)\n",
    "      img = imread(img_file)\n",
    "      if img.ndim == 2:\n",
    "        img.shape = (64, 64, 1)\n",
    "      X_val[i] = img.transpose(2, 0, 1)\n",
    "\n",
    "  # Next load test images\n",
    "  # Students won't have test labels, so we need to iterate over files in the\n",
    "  # images directory.\n",
    "  img_files = os.listdir(os.path.join(path, 'test', 'images'))\n",
    "  X_test = np.zeros((len(img_files), 3, 64, 64), dtype=dtype)\n",
    "  for i, img_file in enumerate(img_files):\n",
    "    img_file = os.path.join(path, 'test', 'images', img_file)\n",
    "    img = imread(img_file)\n",
    "    if img.ndim == 2:\n",
    "      img.shape = (64, 64, 1)\n",
    "    X_test[i] = img.transpose(2, 0, 1)\n",
    "\n",
    "  y_test = None\n",
    "  y_test_file = os.path.join(path, 'test', 'test_annotations.txt')\n",
    "  if os.path.isfile(y_test_file):\n",
    "    with open(y_test_file, 'r') as f:\n",
    "      img_file_to_wnid = {}\n",
    "      for line in f:\n",
    "        line = line.split('\\t')\n",
    "        img_file_to_wnid[line[0]] = line[1]\n",
    "    y_test = [wnid_to_label[img_file_to_wnid[img_file]] for img_file in img_files]\n",
    "    y_test = np.array(y_test)\n",
    "  \n",
    "  mean_image = X_train.mean(axis=0)\n",
    "  if subtract_mean:\n",
    "    X_train -= mean_image[None]\n",
    "    X_val -= mean_image[None]\n",
    "    X_test -= mean_image[None]\n",
    "\n",
    "  return {\n",
    "    'class_names': class_names,\n",
    "    'X_train': X_train,\n",
    "    'y_train': y_train,\n",
    "    'X_val': X_val,\n",
    "    'y_val': y_val,\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test,\n",
    "    'class_names': class_names,\n",
    "    'mean_image': mean_image,\n",
    "  }\n",
    "\n",
    "\n",
    "def load_models(models_dir):\n",
    "  \"\"\"\n",
    "  Load saved models from disk. This will attempt to unpickle all files in a\n",
    "  directory; any files that give errors on unpickling (such as README.txt) will\n",
    "  be skipped.\n",
    "\n",
    "  Inputs:\n",
    "  - models_dir: String giving the path to a directory containing model files.\n",
    "    Each model file is a pickled dictionary with a 'model' field.\n",
    "\n",
    "  Returns:\n",
    "  A dictionary mapping model file names to models.\n",
    "  \"\"\"\n",
    "  models = {}\n",
    "  for model_file in os.listdir(models_dir):\n",
    "    with open(os.path.join(models_dir, model_file), 'rb') as f:\n",
    "      try:\n",
    "        models[model_file] = load_pickle(f)['model']\n",
    "      except pickle.UnpicklingError:\n",
    "        continue\n",
    "  return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr,ytr,Xte,yte = load_CIFAR10(\"../../cifar-10-python/cifar-10-batches-py/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = Xtr.astype('uint8')\n",
    "ytr = ytr.astype('uint8')\n",
    "Xte = Xte.astype('uint8')\n",
    "yte = yte.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(Xtr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-36fb9c6f13fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mXtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mkp1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdes1\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0msift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdes1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdes1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "des = []\n",
    "for img in Xtr.astype('uint8'):\n",
    "    kp1,des1 =sift.detectAndCompute(img,None)\n",
    "    if des1 is not None:\n",
    "        des.append(des1)\n",
    "    else:\n",
    "        des.append(np.zeros((1,128)))\n",
    "print(type(des))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(des[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_list = []\n",
    "for i in (des):\n",
    "    for j in i:\n",
    "          des_list.append(j)\n",
    "print(len(des_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(des_list))\n",
    "print(type(des_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=30,random_state=0).fit(des_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(des[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_train = np.zeros((len(des),30))\n",
    "for i in range(len(des)):\n",
    "    run = kmeans.predict(des[i])\n",
    "    for k in run:\n",
    "        hist_train[i][k]+=1\n",
    "print(hist_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_test = []\n",
    "for img in Xte.astype('uint8'):\n",
    "    kp1,des1 =sift.detectAndCompute(img,None)\n",
    "    if des1 is not None:\n",
    "        des_test.append(des1)\n",
    "    else:\n",
    "        des_test.append(np.zeros((1,128)))\n",
    "print(type(des_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_test = np.zeros((len(des_test),30))\n",
    "for i in range(len(des_test)):\n",
    "    run = kmeans.predict(des_test[i])\n",
    "    print(run)\n",
    "    for k in run:\n",
    "        hist_test[i][k]+=1\n",
    "print(hist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = LinearSVC()\n",
    "svm_clf.fit(hist_train,ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypredict = svm_clf.predict(hist_test)\n",
    "print(ypredict)\n",
    "accuracy_score(ypredict,yte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(random_state=0,solver='lbfgs',multi_class='multinomial').fit(hist_train,ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypredict1=lr_clf.predict(hist_test)\n",
    "print(ypredict1)\n",
    "accuracy_score(ypredict1,yte)\n",
    "# pickle_out = open(\"knn_lr_clf_n=30.pickle\",\"wb\")\n",
    "# pickle.dump(lr_clf, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_clf.fit(hist_train,ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypredict2 = knn_clf.predict(hist_test)\n",
    "print(ypredict2)\n",
    "accuracy_score(ypredict2,yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
